{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 103089 - Data mining\n",
    "\n",
    "<center><img src=\"M-UdL2.png\"  width=\"300\" alt=\"Italian Trulli\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1: Computing frequent pairs and association rules from transactions\n",
    "\n",
    "Consider the following programming exercise. Given the information of the frequent singletons $(L_1)$ and frequent pairs $(L_2)$ we compute with our implementation of A-Priori for k=2 (at the notebook you have available), implement in spark functions to compute the confidence and interest of all the binary rules we can build from the set $L_2$. As the dataset to test your code, compute a set of transactions from this data set:\n",
    "\n",
    "https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset  (It is also included as an attachment to this assignment)\n",
    "\n",
    "Observe that in this dataset each line is not a transaction, but a single purchased product in a given date by a given costumer. You must \"aggregate\" all the purchases by the same date and costumer to build a transaction.\n",
    "\n",
    "You can also test your program with this smaller one (that it already contains one transaction per line) if you are not able to work with the previous one (but the maximum grade if only this dataset is used will be a 8):\n",
    "\n",
    "https://www.kaggle.com/shazadudwadia/supermarket?select=GroceryStoreDataSet.csv\n",
    "\n",
    "Try these values for $\\theta$: 0.009, 0.01, 0.012\n",
    "\n",
    "Number of frequent pairs you should get for each value of $\\theta$ with the bigger data set  (Groceries_dataset.csv):\n",
    "\n",
    "- $\\theta$ = 0.009  ->  |L2| = 6\n",
    "- $\\theta$ = 0.01   ->  |L2| = 5\n",
    "- $\\theta$ = 0.012  ->  |L2| = 2\n",
    "\n",
    "Once you have computed the sets T, L1, 1 and L2, your program should follow these steps:\n",
    "\n",
    "Map, using mapPartitions, each frequent pair in the RDD with L2 to its list of binary association rules (two association rules per each different frequent pair). Use then the flattened version of this RDD.\n",
    "Map each association rule of the previous resulting RDD, to a triple with (rule,confidence,interest). Observe that you will need to use the information in L1 and the number of transactions to compute these values. You can use the version of L1 stored as a python list in the driver (so it can be passed inside functions passed to spark tasks).\n",
    "Finally, sort the association rules by their interest, and show back in the driver program the first 10 most interesting rules\n",
    "\n",
    "\n",
    "You must present your program (together with its results after running it with the dataset I ask and with the three values of $\\theta$) in a single python notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Initialization of PySpark\n",
    "\n",
    "This cell initializes PySpark, a Python library for Spark, a big data processing framework. It sets up the environment, specifies the Python version to use, and creates a SparkContext (sc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON3\"] = \"python\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "print (spark_home)\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sc.setLogLevel('ERROR')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Loading Data\n",
    "This cell loads a dataset named Groceries_dataset.csv into a pandas DataFrame (df). It displays the first few rows of the DataFrame to give an overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./Groceries_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Data Cleaning and Grouping\n",
    "We processes a Spark DataFrame by removing duplicates, grouping the data by 'Member_number' and 'Date' while aggregating other columns with comma-separated values, and then further manipulating the 'itemDescription' column to have a cleaner format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates()\n",
    "grouped =  df.groupby(['Member_number', 'Date']).agg(lambda x: ','.join(x)).reset_index()\n",
    "grouped['itemDescription'] = grouped['itemDescription'].str.split(',')\n",
    "grouped['itemDescription'] = grouped['itemDescription'].astype(str).str.replace('[', '').str.replace(']', '').str.replace('\\'', '')\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Creating a List of Items\n",
    "Now we extract the 'itemDescription' column from the grouped DataFrame, which has been previously processed, and then applies the str.split(', ') method to split the comma-separated strings into lists of items. These lists are then converted to a nested list using .tolist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_list = grouped['itemDescription'].str.split(', ').tolist()\n",
    "print(items_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Creating an RDD in Spark\n",
    "rddT.collect() will return a list of lists, where each inner list contains the items associated with a specific combination of 'Member_number' and 'Date' that we previously split and prepared. The data is now available in the driver program for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = pyspark.sql.SparkSession(sc)\n",
    "rddT =  sc.parallelize(items_list)\n",
    "rddT.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Compute $L_1$ and $T_{L_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rdd with frequent singleton sets (L_1)\n",
    "def computeL1 ( rddT, numtrans, theta ):\n",
    "  rddtemp = rddT.flatMap( lambda t : [ (it,1) for it in t ] ).reduceByKey( lambda a,b : a+b  )\n",
    "  return rddtemp.filter( lambda x : (float(x[1])/numtrans) >= theta )\n",
    "\n",
    "# Map any transaction to its version without elements not in L1\n",
    "# L1 must be a python list, not a RDD\n",
    "def computeTfilteredByL1( seqOfT, L1 ):\n",
    "    for t in seqOfT:\n",
    "       yield [ it for it in t if (it in L1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Compute $L_2$ from $C_2(T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each t in seqofFilteredT (they come from T_{L_1}), compute pairs (a,b) from t that belong to C_2\n",
    "def generateC2( seqofFilteredT ):\n",
    "    for t in seqofFilteredT:\n",
    "      cpairslist = []\n",
    "      for (a,b) in [ (a,b) for i,a in enumerate(t[:-1]) for b in t[i+1:] ]:\n",
    "                cpairslist.append( ((a,b),1) if (a <= b) else ((b,a),1)  )         \n",
    "      yield cpairslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Compute $L_2$ from $C_2(T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeL2( rddC2T, numtrans, theta ):\n",
    "    pairsCountedrdd = rddC2T.reduceByKey( lambda v1,v2 : v1+v2 )\n",
    "    # Finally, filter out from the previous rdd those pairs with frequency below theta\n",
    "    return pairsCountedrdd.filter( lambda x : (float(x[1])/numtrans) >= theta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: theta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numtrans = len(items_list)\n",
    "theta = 0.009 # ->  |L2| = 6\n",
    "#theta = 0.01 #   ->  |L2| = 5\n",
    "#theta = 0.012 # ->  |L2| = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: A-Priori algorithm (Phase 1)\n",
    "Compute the RDD for $L_1$ and $T_{L_1}$ and convert it to a python list back to the driver (remember, this may not scale well depending on the size of $L_1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddL1 = computeL1(rddT, numtrans, theta)\n",
    "L1 = rddL1.keys().collect() # we need only the items (keys) in final L1\n",
    "TL1 = rddT.mapPartitions( lambda seqOfT : computeTfilteredByL1( seqOfT, L1 )  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 11: A-Priori algorithm (Phase 2)\n",
    "Compute $C_2(T)$ from $T_{L_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddC2T = TL1.mapPartitions( lambda seqOfFilteredT : generateC2( seqOfFilteredT ) )\n",
    "rddC2TFlat = rddC2T.flatMap( lambda x : x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 12: A-Priori algorithm (Phase 3)\n",
    "Compute ùêø2 from ùê∂2(ùëá)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddL2 = computeL2( rddC2TFlat, numtrans, theta )\n",
    "rddL2 = rddL2.sortBy(lambda a: -a[1])\n",
    "for it in rddL2.toLocalIterator():\n",
    "    print (it)\n",
    "\n",
    "print(rddL2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 13: Finding association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_association_rules(rddL2):\n",
    "    for rule in rddL2:\n",
    "        yield [((rule[0][0], rule[0][1]), rule[1])]\n",
    "        yield [((rule[0][1], rule[0][0]), rule[1])]\n",
    "\n",
    "rules_rdd = rddL2.mapPartitions(generate_association_rules)\n",
    "flatt_rdd = rules_rdd.flatMap(lambda x: list(x))\n",
    "results = flatt_rdd.collect()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 14: Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(support_A,support_B):\n",
    "    return support_A[1] / support_B[1]\n",
    "\n",
    "#confidence(rddL1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 15: Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interest(support_AB,support_A,support_B,long):\n",
    "    return confidence(support_AB,support_A) - (support_B[1] / long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 16: TripleRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tripleRDD(rules,rdd,len):\n",
    "    for item in rdd:\n",
    "        if item[0] == rules[0][0]: x = item[1]\n",
    "        if item[0] == rules[0][1]: y = item[1]\n",
    "\n",
    "    return (rules[0],             # Rule\n",
    "            rules[1] / x,         # Confidence\n",
    "            rules[1] / x - y / len) # Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 17: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddL1list = rddL1.collect()\n",
    "interestConfidence = flatt_rdd.map(lambda rule: tripleRDD(rule, rddL1list,numtrans))\n",
    "sorted_data = interestConfidence.sortBy(lambda x: x[2], ascending=False)\n",
    "first_10_values = sorted_data.take(10)\n",
    "#sorted_data = sorted(first_10_values, key=lambda x: x[2], reverse=True)\n",
    "for value in first_10_values:\n",
    "    print(value)\n",
    "\n",
    "print(interestConfidence.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "- $\\theta$ = 0.009  ->  |L2| = 6\n",
    "\n",
    "(('whole milk', 'yogurt'), 0.07067287346593314, -0.015205626834808694)  \n",
    "(('other vegetables', 'soda'), 0.07936507936507936, -0.01774111591661548)  \n",
    "(('whole milk', 'rolls/buns'), 0.08844688954718578, -0.021557788659056276)  \n",
    "(('soda', 'other vegetables'), 0.09979353062629043, -0.022307652291573637)  \n",
    "(('whole milk', 'soda'), 0.07363520947947524, -0.023470985802219596)  \n",
    "(('other vegetables', 'rolls/buns'), 0.08648056923918993, -0.023524108967052135)  \n",
    "(('rolls/buns', 'other vegetables'), 0.0959902794653706, -0.026110903452493464)  \n",
    "(('yogurt', 'whole milk'), 0.12996108949416343, -0.027961786934360244)  \n",
    "(('whole milk', 'other vegetables'), 0.09394837071519255, -0.028152812202671518)  \n",
    "(('rolls/buns', 'whole milk'), 0.12697448359659783, -0.030948392831925853)  \n",
    "\n",
    "\n",
    "- $\\theta$ = 0.01   ->  |L2| = 5\n",
    "\n",
    "(('whole milk', 'yogurt'), 0.07067287346593314, -0.015205626834808694)  \n",
    "(('whole milk', 'rolls/buns'), 0.08844688954718578, -0.021557788659056276)  \n",
    "(('whole milk', 'soda'), 0.07363520947947524, -0.023470985802219596)  \n",
    "(('other vegetables', 'rolls/buns'), 0.08648056923918993, -0.023524108967052135)  \n",
    "(('rolls/buns', 'other vegetables'), 0.0959902794653706, -0.026110903452493464)  \n",
    "(('yogurt', 'whole milk'), 0.12996108949416343, -0.027961786934360244)  \n",
    "(('whole milk', 'other vegetables'), 0.09394837071519255, -0.028152812202671518)  \n",
    "(('rolls/buns', 'whole milk'), 0.12697448359659783, -0.030948392831925853)  \n",
    "(('other vegetables', 'whole milk'), 0.12151067323481117, -0.03641220319371251)  \n",
    "(('soda', 'whole milk'), 0.11975223675154852, -0.03817063967697516)  \n",
    "\n",
    "- $\\theta$ = 0.012  ->  |L2| = 2\n",
    "\n",
    "(('whole milk', 'rolls/buns'), 0.08844688954718578, -0.021557788659056276)  \n",
    "(('whole milk', 'other vegetables'), 0.09394837071519255, -0.028152812202671518)  \n",
    "(('rolls/buns', 'whole milk'), 0.12697448359659783, -0.030948392831925853)  \n",
    "(('other vegetables', 'whole milk'), 0.12151067323481117, -0.03641220319371251)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
