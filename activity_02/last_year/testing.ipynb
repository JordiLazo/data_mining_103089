{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBrtwN7l_4Tc"
      },
      "source": [
        "# Second Exercise: Cosine Similarity for movie comparison\n",
        "\n",
        "## Exercise statement\n",
        "\n",
        "In this exercise you have to implement in a python notebook using the spark framework:\n",
        "\n",
        "1. The distributed (map/reduce) algorithm of slide \"3.7\" (in notebook \"8-Item-to-Items-globalfiltering-recommenders-py3-sshow.ipynb\")\n",
        "for computing the cosine similarity of a set of products with negative and positive ratings, using as input information an RDD (or spark dataframe that is also distributed) with ratings with this format:\n",
        "\n",
        "     (userID,movieID,rating)\n",
        "\n",
        "2. The computation of the Cosine Similarity (with the previous algorithm) of all the pairs of movies from the different files you have with this exercise:\n",
        "  filtered50movies.csv filtered100movies.csv  filtered150movies.csv   filtered200movies.csv\n",
        "\n",
        "Each file contains ratings for a different set of movies, but the ones in a smaller file\n",
        "are always a subset of a file with bigger size. We provide files with different size\n",
        "in case you have some memory issues in your computer, so use the biggest file you are able to use, although during \"testing\" of your code you can of course use the smallest file, or even any smaller subset of the file filtered50movies.csv.\n",
        "\n",
        "3. Show on the screen the information for the \"top 10\" most similar pairs, but using the\n",
        "name of the movies you can find in the file movies.\n",
        "\n",
        "All the steps should be implemented always with map/reduce operations with spark RDDs/dataframes. Except the last step, when you have to find the name of the movies in the top-ten recommendations.\n",
        "\n",
        "Present your notebook with plenty of comments in all your functions.\n",
        "\n",
        "NOTE: The ratings for movies come from a dataset obtained from the smallest dataset from:\n",
        "https://grouplens.org/datasets/movielens/\n",
        "But the ratings have been re-scaled from the range [0,5] to the range [-3,2.5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxIDcF1Ko5MK"
      },
      "source": [
        "# Project initialization\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "We make sure the dependencies that we will use throughout this exercise are downloaded and installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TKqWyZ1O_4Th",
        "outputId": "141be544-5c40-42c7-b821-5d41468608d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pyspark in /Users/jordilazo/Library/Python/3.9/lib/python/site-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /Users/jordilazo/Library/Python/3.9/lib/python/site-packages (from pyspark) (0.10.9.7)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF3L75H9_4Tk"
      },
      "source": [
        "## Spark context \n",
        "We initialize the Spark context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rvStokvt_4Tl",
        "outputId": "a7e1f415-2b0a-4eeb-c5d5-e646d9246735"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/11/21 21:51:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/11/21 21:51:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://jordis-mbp.home:4041\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x10baf38b0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "SPARK_ENDPOINT = \"local[*]\"\n",
        "sparkSession = SparkSession.builder \\\n",
        "    .master(SPARK_ENDPOINT) \\\n",
        "    .getOrCreate()\n",
        "sparkContext = sparkSession.sparkContext\n",
        "sparkSession\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJA_m-cd_4Tm"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We start by loading the CSV file that contains the information about each movie:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yH3BTkbB_4Tn",
        "outputId": "1dd2c976-acab-4d1e-e177-ab009ec496fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+--------------------+\n",
            "|movieId|               title|              genres|\n",
            "+-------+--------------------+--------------------+\n",
            "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
            "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
            "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
            "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
            "|      5|Father of the Bri...|              Comedy|\n",
            "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
            "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
            "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
            "|      9| Sudden Death (1995)|              Action|\n",
            "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
            "|     11|American Presiden...|Comedy|Drama|Romance|\n",
            "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
            "|     13|        Balto (1995)|Adventure|Animati...|\n",
            "|     14|        Nixon (1995)|               Drama|\n",
            "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
            "|     16|       Casino (1995)|         Crime|Drama|\n",
            "|     17|Sense and Sensibi...|       Drama|Romance|\n",
            "|     18|   Four Rooms (1995)|              Comedy|\n",
            "|     19|Ace Ventura: When...|              Comedy|\n",
            "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
            "+-------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
        "\n",
        "\n",
        "MOVIES_INPUT_PATH = \"movies.csv\"\n",
        "moviesDataFrame = sparkSession.read.csv(MOVIES_INPUT_PATH, header = True)\n",
        "moviesDataFrame.show()\n",
        "moviesRdd = moviesDataFrame.rdd.map(lambda x: (int(x[0]), str(x[1]), str(x[2])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi1w2PKr_4To"
      },
      "source": [
        "We then load the CSV file with the movie ratings: \n",
        "\n",
        "_(for alternative file sizes, check the `Alternative file sizes` folder inside the root of the delivery folder)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wWv7JZyV_4To",
        "outputId": "72ff8a71-6357-42f5-b573-7c08d3ad8ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+------+\n",
            "|UserID|MovieID|Rating|\n",
            "+------+-------+------+\n",
            "|     1|      1|   1.5|\n",
            "|     5|      1|   1.5|\n",
            "|     7|      1|   2.0|\n",
            "|    15|      1|  -0.5|\n",
            "|    17|      1|   2.0|\n",
            "|    18|      1|   1.0|\n",
            "|    19|      1|   1.5|\n",
            "|    21|      1|   1.0|\n",
            "|    27|      1|   0.5|\n",
            "|    31|      1|   2.5|\n",
            "|    32|      1|   0.5|\n",
            "|    33|      1|   0.5|\n",
            "|    40|      1|   2.5|\n",
            "|    43|      1|   2.5|\n",
            "|    44|      1|   0.5|\n",
            "|    45|      1|   1.5|\n",
            "|    46|      1|   2.5|\n",
            "|    50|      1|   0.5|\n",
            "|    54|      1|   0.5|\n",
            "|    57|      1|   2.5|\n",
            "+------+-------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "PythonRDD[36] at RDD at PythonRDD.scala:53\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import FloatType, IntegerType\n",
        "\n",
        "\n",
        "INPUT_PATH = \"filtered50movies.csv\"\n",
        "dataFrame = sparkSession.read.csv(INPUT_PATH, header = True)\n",
        "dataFrame.show()\n",
        "rdd = dataFrame.rdd.map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\n",
        "print(rdd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blmuJZIE_4Tq"
      },
      "source": [
        "# Calculation of the cosine distance for all pairs\n",
        "\n",
        "We start by generating every pair of user-product ratings in the following format:\n",
        "\n",
        "$$  (u,p_1,r_1),(u,p_2,r_2)  $$\n",
        "\n",
        "_(We have avoided carrying redundant information by discarding bidirectional repetitions of pairs)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XfMZWkNP_4Ts",
        "outputId": "7483bf4e-07a0-4037-9c2f-526177de88ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 5:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------------+\n",
            "|(u, p1, r1)|  (u, p2, r2)|\n",
            "+-----------+-------------+\n",
            "|{1, 1, 1.5}|  {1, 3, 1.5}|\n",
            "|{1, 1, 1.5}|  {1, 6, 1.5}|\n",
            "|{1, 1, 1.5}| {1, 47, 2.5}|\n",
            "|{1, 1, 1.5}| {1, 50, 2.5}|\n",
            "|{1, 1, 1.5}| {1, 70, 0.5}|\n",
            "|{1, 1, 1.5}|{1, 110, 1.5}|\n",
            "|{1, 1, 1.5}|{1, 151, 2.5}|\n",
            "|{1, 1, 1.5}|{1, 163, 2.5}|\n",
            "|{1, 1, 1.5}|{1, 216, 2.5}|\n",
            "|{1, 1, 1.5}|{1, 223, 0.5}|\n",
            "|{1, 1, 1.5}|{1, 231, 2.5}|\n",
            "|{1, 1, 1.5}|{1, 235, 1.5}|\n",
            "|{1, 1, 1.5}|{1, 260, 2.5}|\n",
            "|{1, 1, 1.5}|{1, 296, 0.5}|\n",
            "|{1, 1, 1.5}|{1, 316, 0.5}|\n",
            "|{1, 1, 1.5}|{1, 333, 2.5}|\n",
            "|{1, 1, 1.5}|{1, 349, 1.5}|\n",
            "|{1, 1, 1.5}|{1, 356, 1.5}|\n",
            "|{1, 1, 1.5}|{1, 362, 2.5}|\n",
            "|{1, 1, 1.5}|{1, 367, 1.5}|\n",
            "+-----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "cartesianRdd = rdd.cartesian(rdd) \\\n",
        "    .map(lambda x: (\n",
        "        (x[0][0], x[0][1], x[0][2]), \n",
        "        (x[1][0], x[1][1], x[1][2])\n",
        "    )) \\\n",
        "    .filter(lambda x: x[0][0] == x[1][0]) \\\n",
        "    .filter(lambda x: x[0][1] < x[1][1])\n",
        "cartesianDataFrame = sparkSession.createDataFrame(cartesianRdd, [\"(u, p1, r1)\", \"(u, p2, r2)\"])\n",
        "cartesianDataFrame.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxuQbzjE_4Tt"
      },
      "source": [
        "We follow by mapping every pair of user-product ratings with the same user (u) to the values they contribute to in the final cosine distance between p1 and p2:\n",
        "$$  (u,p_1,r_1),(u,p_2,r_2) \\rightarrow ((p_1,p_2),(r_1 r_2,r_1^2,r_2^2) ) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vJuNFrWH_4Tu",
        "outputId": "690d8dbf-714f-40d1-a89a-51b4c655caca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---------------------+\n",
            "|(p1, p2)|(r1 * r2, r1^2, r2^2)|\n",
            "+--------+---------------------+\n",
            "|  {1, 3}|   {2.25, 2.25, 2.25}|\n",
            "|  {1, 6}|   {2.25, 2.25, 2.25}|\n",
            "| {1, 47}|   {3.75, 2.25, 6.25}|\n",
            "| {1, 50}|   {3.75, 2.25, 6.25}|\n",
            "| {1, 70}|   {0.75, 2.25, 0.25}|\n",
            "|{1, 110}|   {2.25, 2.25, 2.25}|\n",
            "|{1, 151}|   {3.75, 2.25, 6.25}|\n",
            "|{1, 163}|   {3.75, 2.25, 6.25}|\n",
            "|{1, 216}|   {3.75, 2.25, 6.25}|\n",
            "|{1, 223}|   {0.75, 2.25, 0.25}|\n",
            "|{1, 231}|   {3.75, 2.25, 6.25}|\n",
            "|{1, 235}|   {2.25, 2.25, 2.25}|\n",
            "|{1, 260}|   {3.75, 2.25, 6.25}|\n",
            "|{1, 296}|   {0.75, 2.25, 0.25}|\n",
            "|{1, 316}|   {0.75, 2.25, 0.25}|\n",
            "|{1, 333}|   {3.75, 2.25, 6.25}|\n",
            "|{1, 349}|   {2.25, 2.25, 2.25}|\n",
            "|{1, 356}|   {2.25, 2.25, 2.25}|\n",
            "|{1, 362}|   {3.75, 2.25, 6.25}|\n",
            "|{1, 367}|   {2.25, 2.25, 2.25}|\n",
            "+--------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from math import pow\n",
        "\n",
        "\n",
        "userProductRatingsRdd = cartesianRdd.map(lambda x: (\n",
        "    (x[0][1], x[1][1]), \n",
        "    (x[0][2] * x[1][2], pow(x[0][2], 2), pow(x[1][2], 2))\n",
        "))\n",
        "userProductRatingsDataFrame = sparkSession.createDataFrame(userProductRatingsRdd, [\"(p1, p2)\", \"(r1 * r2, r1^2, r2^2)\"])\n",
        "userProductRatingsDataFrame.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNWIChiG_4Tu"
      },
      "source": [
        "We then reduce all the previous key-value pairs, with the same key as:\n",
        "$$ ((p_1,p_2),(pra_{1,2},ra_1^2,ra_2^2) ) + ((p_1,p_2),(prb_{1,2},rb_1^2,rb_2^2) ) \n",
        "   \\rightarrow \\\\  ((p_1,p_2),( pra_{1,2}+prb_{1,2}, ra_1^2+rb_1^2,\n",
        "   ra_2^2+rb_2^2) ) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VC_lbC6R_4Tu",
        "outputId": "0045f415-268c-444c-9a76-c0b4974dee29"
      },
      "outputs": [],
      "source": [
        "step2Rdd = userProductRatingsRdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
        "step2DataFrame = sparkSession.createDataFrame(step2Rdd, [\"(p1, p2)\", \"(pra1,2 + prb1,2, ra1^2 + rb1^2, ra2^2 + rb2^2)\"])\n",
        "step2DataFrame.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_WKxw1H_4Tv"
      },
      "source": [
        "We end up computing the cosine distance combining the reduced values in a final map:\n",
        "$$ ((p_1,p_2),(\\sum_u r_1 r_2,\\sum_u r_1^2,\\sum_u r_2^2) ) \\rightarrow \n",
        "\\frac{\\sum_u r_1 r_2}{\\sqrt{\\sum_u r_1^2} \\sqrt{\\sum_u r_2^2}}  $$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVS8Pi6B_4Tv",
        "outputId": "2d76e8dd-2a4a-44d3-da75-4cd8b4394033"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "\n",
        "cosineRdd = step2Rdd.map(lambda x: (\n",
        "    x[0][0], \n",
        "    x[0][1], \n",
        "    x[1][0] / ( sqrt(x[1][1]) * sqrt(x[1][2]) )\n",
        "))\n",
        "cosineDataFrame = sparkSession.createDataFrame(cosineRdd, [\"Movie1Id\", \"Movie2Id\", \"CosineDistance\"])\n",
        "cosineDataFrame.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3inoO2q_4Tw"
      },
      "source": [
        "# Results \n",
        "\n",
        "We get the movie titles matching every movie ID to ease the visualization of the pairs of movies, as requested in the activity statement. \n",
        "\n",
        "We then sort the pairs of movies by their cosine distance and take the first 10 with the highest cosine distance, hence displaying the top 10 most similar pairs of movies, as requested in the activity statement:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYidVb-p_4Ty",
        "outputId": "dfb486c3-1ffc-4f81-9c16-5c7bbb570c2a"
      },
      "outputs": [],
      "source": [
        "movies1DataFrame = sparkSession.createDataFrame(moviesRdd, [\"MovieId1\", \"Title1\", \"Genres1\"])\n",
        "movies2DataFrame = sparkSession.createDataFrame(moviesRdd, [\"MovieId2\", \"Title2\", \"Genres2\"])\n",
        "cosineWithTitlesRdd = cosineDataFrame \\\n",
        "    .join(movies1DataFrame, cosineDataFrame.Movie1Id == movies1DataFrame.MovieId1, \"inner\") \\\n",
        "    .join(movies2DataFrame, cosineDataFrame.Movie2Id == movies2DataFrame.MovieId2, \"inner\") \\\n",
        "    .rdd \\\n",
        "    .map(lambda x: (x[0], x[4], x[1], x[7], x[2])) \\\n",
        "    .sortBy(lambda x: -x[4])\n",
        "cosineWithTitlesDataFrame = sparkSession.createDataFrame(cosineWithTitlesRdd, [\"MovieId1\", \"MovieTitle1\", \"MovieId2\", \"MovieTitle2\", \"CosineDistance\"])\n",
        "cosineWithTitlesDataFrame.show(100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "actividad2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
