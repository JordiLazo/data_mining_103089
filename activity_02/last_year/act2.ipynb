{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBrtwN7l_4Tc"
      },
      "source": [
        "# Second Exercise: Cosine Similarity for movie comparison\n",
        "\n",
        "## Exercise statement\n",
        "\n",
        "In this exercise you have to implement in a python notebook using the spark framework:\n",
        "\n",
        "1. The distributed (map/reduce) algorithm of slide \"3.7\" (in notebook \"8-Item-to-Items-globalfiltering-recommenders-py3-sshow.ipynb\")\n",
        "for computing the cosine similarity of a set of products with negative and positive ratings, using as input information an RDD (or spark dataframe that is also distributed) with ratings with this format:\n",
        "\n",
        "     (userID,movieID,rating)\n",
        "\n",
        "2. The computation of the Cosine Similarity (with the previous algorithm) of all the pairs of movies from the different files you have with this exercise:\n",
        "  filtered50movies.csv filtered100movies.csv  filtered150movies.csv   filtered200movies.csv\n",
        "\n",
        "Each file contains ratings for a different set of movies, but the ones in a smaller file\n",
        "are always a subset of a file with bigger size. We provide files with different size\n",
        "in case you have some memory issues in your computer, so use the biggest file you are able to use, although during \"testing\" of your code you can of course use the smallest file, or even any smaller subset of the file filtered50movies.csv.\n",
        "\n",
        "3. Show on the screen the information for the \"top 10\" most similar pairs, but using the\n",
        "name of the movies you can find in the file movies.\n",
        "\n",
        "All the steps should be implemented always with map/reduce operations with spark RDDs/dataframes. Except the last step, when you have to find the name of the movies in the top-ten recommendations.\n",
        "\n",
        "Present your notebook with plenty of comments in all your functions.\n",
        "\n",
        "NOTE: The ratings for movies come from a dataset obtained from the smallest dataset from:\n",
        "https://grouplens.org/datasets/movielens/\n",
        "But the ratings have been re-scaled from the range [0,5] to the range [-3,2.5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxIDcF1Ko5MK"
      },
      "source": [
        "# Project initialization\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "We make sure the dependencies that we will use throughout this exercise are downloaded and installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKqWyZ1O_4Th",
        "outputId": "141be544-5c40-42c7-b821-5d41468608d5"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF3L75H9_4Tk"
      },
      "source": [
        "## Spark context \n",
        "We initialize the Spark context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvStokvt_4Tl",
        "outputId": "a7e1f415-2b0a-4eeb-c5d5-e646d9246735"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "SPARK_ENDPOINT = \"local[*]\"\n",
        "sparkSession = SparkSession.builder \\\n",
        "    .master(SPARK_ENDPOINT) \\\n",
        "    .getOrCreate()\n",
        "sparkContext = sparkSession.sparkContext\n",
        "sparkSession\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJA_m-cd_4Tm"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We start by loading the CSV file that contains the information about each movie:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH3BTkbB_4Tn",
        "outputId": "1dd2c976-acab-4d1e-e177-ab009ec496fd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
        "\n",
        "\n",
        "MOVIES_INPUT_PATH = \"movies.csv\"\n",
        "moviesDataFrame = sparkSession.read.csv(MOVIES_INPUT_PATH, header = True) \\\n",
        "    .withColumn(\"MovieId\", col(\"MovieId\").cast(IntegerType())) \\\n",
        "    .withColumn(\"Title\", col(\"Title\").cast(StringType())) \\\n",
        "    .withColumn(\"Genres\", col(\"Genres\").cast(StringType()))\n",
        "moviesDataFrame.show()\n",
        "moviesRdd = moviesDataFrame.rdd.map(lambda x: (int(x[0]), str(x[1]), str(x[2])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi1w2PKr_4To"
      },
      "source": [
        "We then load the CSV file with the movie ratings: \n",
        "\n",
        "_(for alternative file sizes, check the `Alternative file sizes` folder inside the root of the delivery folder)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWv7JZyV_4To",
        "outputId": "72ff8a71-6357-42f5-b573-7c08d3ad8ded"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import FloatType, IntegerType\n",
        "\n",
        "\n",
        "INPUT_PATH = \"filtered50movies.csv\"\n",
        "dataFrame = sparkSession.read.csv(INPUT_PATH, header = True) \\\n",
        "    .withColumn(\"UserId\", col(\"UserId\").cast(IntegerType())) \\\n",
        "    .withColumn(\"MovieId\", col(\"MovieId\").cast(IntegerType())) \\\n",
        "    .withColumn(\"Rating\", col(\"Rating\").cast(FloatType()))\n",
        "dataFrame.show()\n",
        "rdd = dataFrame.rdd.map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\n",
        "print(rdd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blmuJZIE_4Tq"
      },
      "source": [
        "# Calculation of the cosine distance for all pairs\n",
        "\n",
        "We start by generating every pair of user-product ratings in the following format:\n",
        "\n",
        "$$  (u,p_1,r_1),(u,p_2,r_2)  $$\n",
        "\n",
        "_(We have avoided carrying redundant information by discarding bidirectional repetitions of pairs)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfMZWkNP_4Ts",
        "outputId": "7483bf4e-07a0-4037-9c2f-526177de88ba"
      },
      "outputs": [],
      "source": [
        "cartesianRdd = rdd.cartesian(rdd) \\\n",
        "    .map(lambda x: (\n",
        "        (x[0][0], x[0][1], x[0][2]), \n",
        "        (x[1][0], x[1][1], x[1][2])\n",
        "    )) \\\n",
        "    .filter(lambda x: x[0][0] == x[1][0]) \\\n",
        "    .filter(lambda x: x[0][1] < x[1][1])\n",
        "cartesianDataFrame = sparkSession.createDataFrame(cartesianRdd, [\"(u, p1, r1)\", \"(u, p2, r2)\"])\n",
        "cartesianDataFrame.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxuQbzjE_4Tt"
      },
      "source": [
        "We follow by mapping every pair of user-product ratings with the same user (u) to the values they contribute to in the final cosine distance between p1 and p2:\n",
        "$$  (u,p_1,r_1),(u,p_2,r_2) \\rightarrow ((p_1,p_2),(r_1 r_2,r_1^2,r_2^2) ) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJuNFrWH_4Tu",
        "outputId": "690d8dbf-714f-40d1-a89a-51b4c655caca"
      },
      "outputs": [],
      "source": [
        "from math import pow\n",
        "\n",
        "\n",
        "userProductRatingsRdd = cartesianRdd.map(lambda x: (\n",
        "    (x[0][1], x[1][1]), \n",
        "    (x[0][2] * x[1][2], pow(x[0][2], 2), pow(x[1][2], 2))\n",
        "))\n",
        "userProductRatingsDataFrame = sparkSession.createDataFrame(userProductRatingsRdd, [\"(p1, p2)\", \"(r1 * r2, r1^2, r2^2)\"])\n",
        "userProductRatingsDataFrame.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNWIChiG_4Tu"
      },
      "source": [
        "We then reduce all the previous key-value pairs, with the same key as:\n",
        "$$ ((p_1,p_2),(pra_{1,2},ra_1^2,ra_2^2) ) + ((p_1,p_2),(prb_{1,2},rb_1^2,rb_2^2) ) \n",
        "   \\rightarrow \\\\  ((p_1,p_2),( pra_{1,2}+prb_{1,2}, ra_1^2+rb_1^2,\n",
        "   ra_2^2+rb_2^2) ) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC_lbC6R_4Tu",
        "outputId": "0045f415-268c-444c-9a76-c0b4974dee29"
      },
      "outputs": [],
      "source": [
        "step2Rdd = userProductRatingsRdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
        "step2DataFrame = sparkSession.createDataFrame(step2Rdd, [\"(p1, p2)\", \"(pra1,2 + prb1,2, ra1^2 + rb1^2, ra2^2 + rb2^2)\"])\n",
        "step2DataFrame.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_WKxw1H_4Tv"
      },
      "source": [
        "We end up computing the cosine distance combining the reduced values in a final map:\n",
        "$$ ((p_1,p_2),(\\sum_u r_1 r_2,\\sum_u r_1^2,\\sum_u r_2^2) ) \\rightarrow \n",
        "\\frac{\\sum_u r_1 r_2}{\\sqrt{\\sum_u r_1^2} \\sqrt{\\sum_u r_2^2}}  $$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVS8Pi6B_4Tv",
        "outputId": "2d76e8dd-2a4a-44d3-da75-4cd8b4394033"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "\n",
        "cosineRdd = step2Rdd.map(lambda x: (\n",
        "    x[0][0], \n",
        "    x[0][1], \n",
        "    x[1][0] / ( sqrt(x[1][1]) * sqrt(x[1][2]) )\n",
        "))\n",
        "cosineDataFrame = sparkSession.createDataFrame(cosineRdd, [\"Movie1Id\", \"Movie2Id\", \"CosineDistance\"])\n",
        "cosineDataFrame.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3inoO2q_4Tw"
      },
      "source": [
        "# Results \n",
        "\n",
        "We get the movie titles matching every movie ID to ease the visualization of the pairs of movies, as requested in the activity statement. \n",
        "\n",
        "We then sort the pairs of movies by their cosine distance and take the first 10 with the highest cosine distance, hence displaying the top 10 most similar pairs of movies, as requested in the activity statement:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYidVb-p_4Ty",
        "outputId": "dfb486c3-1ffc-4f81-9c16-5c7bbb570c2a"
      },
      "outputs": [],
      "source": [
        "movies1DataFrame = sparkSession.createDataFrame(moviesRdd, [\"MovieId1\", \"Title1\", \"Genres1\"])\n",
        "movies2DataFrame = sparkSession.createDataFrame(moviesRdd, [\"MovieId2\", \"Title2\", \"Genres2\"])\n",
        "cosineWithTitlesRdd = cosineDataFrame \\\n",
        "    .join(movies1DataFrame, cosineDataFrame.Movie1Id == movies1DataFrame.MovieId1, \"inner\") \\\n",
        "    .join(movies2DataFrame, cosineDataFrame.Movie2Id == movies2DataFrame.MovieId2, \"inner\") \\\n",
        "    .rdd \\\n",
        "    .map(lambda x: (x[0], x[4], x[1], x[7], x[2])) \\\n",
        "    .sortBy(lambda x: -x[4])\n",
        "cosineWithTitlesDataFrame = sparkSession.createDataFrame(cosineWithTitlesRdd, [\"MovieId1\", \"MovieTitle1\", \"MovieId2\", \"MovieTitle2\", \"CosineDistance\"])\n",
        "cosineWithTitlesDataFrame.show(100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "actividad2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
