{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 103089 - Data mining\n",
    "\n",
    "<center><img src=\"media/M-UdL2.png\"  width=\"300\" alt=\"Universitat de Lleida\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2: Cosine similarity for movie comparison \n",
    "\n",
    "In this exercise you have to implement in a python notebook using the spark framework:\n",
    "\n",
    "1. The distributed (map/reduce) algorithm of slide \"3.7\" (in notebook \"8-Item-to-Items-globalfiltering-recommenders-py3-sshow.ipynb\") for computing the cosine similarity of a set of products with negative and positive ratings, using as input information an RDD (or spark dataframe that is also distributed) with ratings with this format:\n",
    "\n",
    "- (userID,movieID,rating)\n",
    "\n",
    "2. The computation of the Cosine Similarity (with the previous algorithm) of all the pairs of movies from the different files you have with this exercise:\n",
    "\n",
    "- filtered50movies.csv\n",
    "\n",
    "- filtered100movies.csv\n",
    "\n",
    "- filtered150movies.csv\n",
    "\n",
    "- filtered200movies.csv\n",
    "\n",
    "Each file contains ratings for a different set of movies, but the ones in a smaller file are always a subset of a file with bigger size. We provide files with different size in case you have some memory issues in your computer, so use the biggest file you are able to use, although during \"testing\" of your code you can of course use the smallest file, or even any smaller subset of the file filtered50movies.csv.\n",
    "\n",
    "3. Show on the screen the information for the \"top 10\" most similar pairs, but using the name of the movies you can find in the  movies file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Initialization of PySpark\n",
    "\n",
    "This cell initializes PySpark, a Python library for Spark, a big data processing framework. It sets up the environment, specifies the Python version to use, and creates a SparkContext (sc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "import math\n",
    "\n",
    "SPARK_ENDPOINT = \"local[*]\"\n",
    "sparkSession = pyspark.sql.SparkSession.builder.master(SPARK_ENDPOINT).getOrCreate()\n",
    "sparkContext = sparkSession.sparkContext\n",
    "sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Loading Data (movies.csv)\n",
    "We load a dataset named movies.csv, process the data to change the data types of certain columns, convert it into an RDD for distributed processing, and then print the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDataFrame = sparkSession.read.csv(\"dataset/movies.csv\", header = True)\n",
    "moviesDataFrame.show()\n",
    "moviesRdd = moviesDataFrame.rdd.map(lambda x: (int(x[0]), str(x[1]), str(x[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Loading Data (filtered50movies.csv)\n",
    "We load a dataset named filtered50movies.csv, process the data to change the data types of certain columns, convert it into an RDD for distributed processing, and then print both the schema of the DataFrame and the content of the RDD to the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredMoviesDataFrame = sparkSession.read.csv(\"dataset/filtered50movies.csv\", header = True)\n",
    "filteredRdd = filteredMoviesDataFrame.rdd.map(lambda x: (int(x[0]), int(x[1]), float(x[2])))\n",
    "filteredMoviesDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Calculation of the cosine distance for all pairs\n",
    "We perform the Cartesian product and process the pairs based on certain conditions, and then we create a DataFrame from the processed RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartesian_rdd = filteredRdd.cartesian(filteredRdd)\n",
    "\n",
    "def process_pair(pair):\n",
    "    (user1, product1, rating1), (user2, product2, rating2) = pair\n",
    "    return ((user1, product1, rating1), (user2, product2, rating2))\n",
    "\n",
    "def filter_pair(pair):\n",
    "    (user1, product1, _), (user2, product2, _) = pair\n",
    "    return user1 == user2 and product1 < product2\n",
    "\n",
    "processed_cartesian_rdd = cartesian_rdd.map(process_pair).filter(filter_pair)\n",
    "\n",
    "cartesian_dataframe = sparkSession.createDataFrame(processed_cartesian_rdd, [\"(u, p1, r1)\", \"(u, p2, r2)\"])\n",
    "\n",
    "cartesian_dataframe.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Mapping user-product ratings\n",
    "We map every pair of user-product ratings with the same user (u) to the values they contribute to in the final cosine distance between p1 and p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userProductRatingsRdd = processed_cartesian_rdd.map(lambda x: (\n",
    "    (x[0][1], x[1][1]), \n",
    "    (x[0][2] * x[1][2], pow(x[0][2], 2), pow(x[1][2], 2))\n",
    "))\n",
    "userProductRatingsDataFrame = sparkSession.createDataFrame(userProductRatingsRdd, [\"(p1, p2)\", \"(r1 * r2, r1^2, r2^2)\"])\n",
    "userProductRatingsDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Reduce key-value pairs\n",
    "Then we reduce all the previous key-value pairs with the same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2Rdd = userProductRatingsRdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "step2DataFrame = sparkSession.createDataFrame(step2Rdd, [\"(p1, p2)\", \"(pra1,2 + prb1,2, ra1^2 + rb1^2, ra2^2 + rb2^2)\"])\n",
    "step2DataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Compute cosine distance\n",
    "We end up calculating the cosine similarity between pairs of movies based on their ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_distance(data):\n",
    "    movie1_id, movie2_id = data[0]\n",
    "    dot_product, sum_squares1, sum_squares2 = data[1]\n",
    "    cosine_distance = dot_product / (math.sqrt(sum_squares1) * math.sqrt(sum_squares2))\n",
    "    return (movie1_id, movie2_id, cosine_distance)\n",
    "\n",
    "cosineRdd = step2Rdd.map(calculate_cosine_distance)\n",
    "cosineDataFrame = sparkSession.createDataFrame(cosineRdd, [\"Movie1Id\", \"Movie2Id\", \"CosineDistance\"])\n",
    "cosineDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Print results \n",
    "\n",
    "We sort the pairs of movies by their cosine distance and take the first 10 with the highest cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies1_df = sparkSession.createDataFrame(moviesRdd, [\"MovieId1\", \"Title1\", \"Genres1\"])\n",
    "movies2_df = sparkSession.createDataFrame(moviesRdd, [\"MovieId2\", \"Title2\", \"Genres2\"])\n",
    "\n",
    "joined_df = (cosineDataFrame\n",
    "    .join(movies1_df, cosineDataFrame.Movie1Id == movies1_df.MovieId1)\n",
    "    .join(movies2_df, cosineDataFrame.Movie2Id == movies2_df.MovieId2))\n",
    "\n",
    "cosine_with_titles_df = (joined_df\n",
    "    .select(cosineDataFrame.Movie1Id, movies1_df.Title1, cosineDataFrame.Movie2Id, movies2_df.Title2, cosineDataFrame.CosineDistance)\n",
    "    .withColumnRenamed(\"Title1\", \"MovieTitle1\")\n",
    "    .withColumnRenamed(\"Title2\", \"MovieTitle2\")\n",
    "    .orderBy(cosineDataFrame.CosineDistance.desc()))\n",
    "\n",
    "cosine_with_titles_df.show(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
